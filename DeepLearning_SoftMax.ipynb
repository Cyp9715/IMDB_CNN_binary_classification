{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tt4154756\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-a4f556e0a389>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[0mtconst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"tt4154756\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[0mgenres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"action\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgarbage_remove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtconst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtconst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-a4f556e0a389>\u001b[0m in \u001b[0;36mpreprocessing\u001b[1;34m(garbage_remove)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mtest_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import csv\n",
    "import numpy\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sequenceGenres():\n",
    "    f = open(r'./Basic_DATA/Movie_Genres.tsv', 'r', encoding='utf-8')\n",
    "    rdr = csv.reader(f, delimiter='\\t')\n",
    "\n",
    "    #영화, 출력 tconst, 출력 장르값\n",
    "    movie, output_tconst, output_genres = [] ,[], []\n",
    "    for i in rdr:\n",
    "        movie.append(i)\n",
    "\n",
    "    for i in range(movie.__len__()):\n",
    "    #   if movie[i][1].find(\"Horror\") == -1 and movie[i][1].find(\"Romance\")>=0:\n",
    "\n",
    "        if movie[i][1].find(\"Horror\") >= 0:\n",
    "            output_tconst.append(movie[i][0])\n",
    "            output_genres.append(movie[i][1])\n",
    "    \n",
    "    print(output_tconst.__len__(), output_genres.__len__())\n",
    "    \n",
    "    return output_tconst, output_genres\n",
    "\n",
    "# 트레인과 테스트 쓸모없는 값을 제거하는 단입니다, 현재 불용어를 제거하지 않은 상태입니다 (Movie, File)\n",
    "# 불용어를 제거하게 되면     t = Tokenizer(num_words=1000) 값을 더 줄이더라도 유의미한 값을 얻을 수 있을 것으로 예상됩니다.\n",
    "# t = Tokenizer(num_words=1000) 값을 줄이게 되면 GPU의 메모리 부하량이 줄어듭니다.\n",
    "#추 후 라이브러리를 통해 불용어를 제거할 예정입니다.\n",
    "def garbage_remove(tconst):\n",
    "    train = pd.read_csv(r'./Action set/Action.csv', skiprows=1, names=['Text', 'sentiment'])\n",
    "    test = pd.read_csv(r\"./Web_Crowling/\" + tconst + \".tsv\",sep='\\t', names=['Rank', 'ID', 'Title','Text','sentiment'])\n",
    "    print(tconst)\n",
    "    A, B = [], []\n",
    "\n",
    "    for i in train['Text']:\n",
    "        i = re.sub('[^a-zA-Z0-9 \\n]', '', str(i))\n",
    "        i = i.replace(\"br\",\"\").replace(\"  \",\" \").replace(\"   \",\" \")\n",
    "        A.append(i)\n",
    "\n",
    "    for i in test['Text']:\n",
    "        i = re.sub('[^a-zA-Z0-9 \\n]', '', str(i))\n",
    "        i = i.replace(\"br\",\"\").replace(\"  \",\" \").replace(\"   \",\" \")\n",
    "        B.append(i)\n",
    "\n",
    "    train['Text'] = A\n",
    "    test['Text'] = B\n",
    "    return train,test\n",
    "\n",
    "\n",
    "#쓰레기 값을 제거한 데이터들에 대해서, 백터화-웟 핫 인코딩을 한 뒤, 반환합니다.\n",
    "def preprocessing(garbage_remove):\n",
    "    t = Tokenizer(num_words=1250)\n",
    "    train,test = garbage_remove\n",
    "    test_label, train_label = [], []\n",
    "\n",
    "    t.fit_on_texts(train['Text'])\n",
    "    t.fit_on_texts(test['Text'])\n",
    "\n",
    "    #cruel = 1, astonish = 0 로 설정합니다.\n",
    "    for i in train['sentiment']:\n",
    "        if i == 'fantasy':\n",
    "            train_label.append(0)\n",
    "        elif i == 'reality':\n",
    "            train_label.append(1)\n",
    "        elif i == 'intelligence':\n",
    "            train_label.append(2)\n",
    "        else:\n",
    "            train_label.append(3)\n",
    "    \n",
    "    #(이 부분 고려 필요) test_label 을 모두 cruel 로 설정, 정확도가 떨어질수록 astonish에 가깝다는 논지로, 상당히 정확합니다.\n",
    "    # 다만 .h5파일로 저장하여 불러들인뒤, 다른 데이터 셋(B)을 추가하여 확인하면\n",
    "    # 다른 데이터셋 (B)를 Test 데이터로 둔 것보다 훨신 떨어지는 결과를 얻습니다.\n",
    "\n",
    "    train_result = t.texts_to_sequences(train['Text'])\n",
    "    test_result = t.texts_to_sequences(test['Text'])\n",
    "\n",
    "    # 원 핫 인코딩\n",
    "    train_result = t.sequences_to_matrix(train_result, mode='binary')\n",
    "    test_result = t.sequences_to_matrix(test_result, mode='binary')\n",
    "    return train_result, train_label\n",
    "\n",
    "#실질적인 모델 구성입니다. CNN 모델로 수성되어 있습니다\n",
    "def AI(preprocessing):\n",
    "    X_train,y_train = preprocessing\n",
    "    num_classes = numpy.max(y_train) + 1\n",
    "\n",
    "\n",
    "    top_words = 2000\n",
    "    max_words = 1250\n",
    "    # amx_words 를 1000 이상으로 설정해 주어야 비교적 정확한 값을 얻을 수 있습니다. 임의적으로 줄이면 그래픽카드 메모리가 적어도 상관은 없지만.\n",
    "    # 30~70 사이의 값은 잔차가 굉장히 많이 튀게 됩니다.\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(max_words,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='accuracy', min_delta=0, patience=3, verbose=0, mode='auto') # 조기종료 콜백함수 정의\n",
    "    hist = model.fit(X_train, y_train,\n",
    "                        batch_size=64,\n",
    "                        epochs=50,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.1)\n",
    "\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def write(tconst, genres, scores):\n",
    "    f = open(r\"./Basic_DATA/Horror.tsv\", 'a', encoding='UTF8')\n",
    "    data = str(tconst) + \"\\t\" + str(genres) + \"\\t\" + \"Horror\" + \"\\t\" + str(numpy.round(scores[1]*100,2)) + \"\\n\"\n",
    "    print(\"data = \",data)\n",
    "    f.write(data)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "\n",
    "# score = AI(preprocessing(garbage_remove()))\n",
    "# TCONST,GENRES = sequenceGenres()\n",
    "\n",
    "tconst = \"tt4154756\"\n",
    "genres = \"action\"\n",
    "scores = AI(preprocessing(garbage_remove(tconst)))\n",
    "keras.backend.clear_session()\n",
    "write(tconst, genres, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                post          tags\n",
      "0  robert ludlum is one of my favorite spy storie...  intelligence\n",
      "1  at the end of his career renegade and brillian...         cruel\n",
      "2  despite the fact that i am a big momoa fan and...       fantasy\n",
      "3  its funny how summer movies play out over time...  intelligence\n",
      "4  deserves more credit and attention but it like...  intelligence\n",
      "5  bryan singer said that this movie was driven b...       fantasy\n",
      "6  i saw the departed before watching and i unfor...  intelligence\n",
      "7  im going to keep it short  some people find a ...       reality\n",
      "8  luc bessons was one of the most stylish action...  intelligence\n",
      "9  i had a lovehate relationship with vol 1 now t...         cruel\n",
      "2493295\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline\n",
    "\n",
    "my_tags = ['fantasy','reality','intelligence','cruel']\n",
    "df = pd.read_csv('./Action set/Action.csv')\n",
    "df = df[pd.notnull(df['tags'])]\n",
    "print(df.head(10))\n",
    "print(df['post'].apply(lambda x: len(x.split(' '))).sum())\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "df.tags.value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summer another action film morgan freeman usually associated action movies fits one latest stint much better dark knight proves film strong dark knight proves action good walks dark knight sprints finish line plot spoiler guardian group operating 1000 years assassinating people according orders mysterious source higher enough hold together excitement film lots action sceneswesley enlisted group early movie told father killed breaks dull humdrum office account manager job exciting world beaten killing peoplehe dumps younger chick girlfriend older tattooed woman angela jolie thats ok young one doesnt really seem interested plenty interested friendsoverall action top notch though wonder car crash sequences much gasoline would cost imagine gas cheaper filmed cast ok many summer action films last years major plot twist wesley late film leave viewers find one good thing plot makes sense even though circumstances stretch sometimes seems go little farthis film ok good flick drivein theaters would fight see action movies back seats major mature language bursts many stimulating simulated sex scenes involving younger chick wesleys best friend brief nothing major\n",
      "Tag: fantasy\n",
      "summer another action film morgan freeman usually associated action movies fits one latest stint much better dark knight proves film strong dark knight proves action good walks dark knight sprints finish line plot spoiler guardian group operating 1000 years assassinating people according orders mysterious source higher enough hold together excitement film lots action sceneswesley enlisted group early movie told father killed breaks dull humdrum office account manager job exciting world beaten killing peoplehe dumps younger chick girlfriend older tattooed woman angela jolie thats ok young one doesnt really seem interested plenty interested friendsoverall action top notch though wonder car crash sequences much gasoline would cost imagine gas cheaper filmed cast ok many summer action films last years major plot twist wesley late film leave viewers find one good thing plot makes sense even though circumstances stretch sometimes seems go little farthis film ok good flick drivein theaters would fight see action movies back seats major mature language bursts many stimulating simulated sex scenes involving younger chick wesleys best friend brief nothing major\n",
      "Tag: fantasy\n"
     ]
    }
   ],
   "source": [
    "# Text Pre-Prosessing \n",
    "\n",
    "def print_plot(index):\n",
    "    example = df[df.index == index][['post', 'tags']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Tag:', example[1])\n",
    "print_plot(10)\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "    \n",
    "df['post'] = df['post'].apply(clean_text)\n",
    "print_plot(10)\n",
    "\n",
    "\n",
    "X = df.post\n",
    "y = df.tags\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.790272614622057\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     fantasy       0.99      0.71      0.82       764\n",
      "     reality       0.90      0.94      0.92      1014\n",
      "intelligence       1.00      0.25      0.40       490\n",
      "       cruel       0.62      0.97      0.76       960\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      3228\n",
      "   macro avg       0.88      0.72      0.73      3228\n",
      "weighted avg       0.85      0.79      0.77      3228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9132589838909542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     fantasy       0.95      0.88      0.91       764\n",
      "     reality       0.90      0.96      0.93      1014\n",
      "intelligence       0.98      0.89      0.93       490\n",
      "       cruel       0.88      0.90      0.89       960\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      3228\n",
      "   macro avg       0.92      0.91      0.92      3228\n",
      "weighted avg       0.92      0.91      0.91      3228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 6777 samples, validate on 754 samples\n",
      "Epoch 1/20\n",
      "6777/6777 [==============================] - 2s 264us/step - loss: 0.5541 - acc: 0.8023 - val_loss: 0.2950 - val_acc: 0.8992\n",
      "Epoch 2/20\n",
      "6777/6777 [==============================] - 1s 219us/step - loss: 0.2386 - acc: 0.9193 - val_loss: 0.2895 - val_acc: 0.8966\n",
      "Epoch 3/20\n",
      "6777/6777 [==============================] - 1s 220us/step - loss: 0.1662 - acc: 0.9447 - val_loss: 0.2930 - val_acc: 0.8952\n",
      "Epoch 4/20\n",
      "6777/6777 [==============================] - 1s 220us/step - loss: 0.1212 - acc: 0.9612 - val_loss: 0.3205 - val_acc: 0.8820\n",
      "Epoch 5/20\n",
      "6777/6777 [==============================] - 2s 224us/step - loss: 0.0919 - acc: 0.9743 - val_loss: 0.3193 - val_acc: 0.8846\n",
      "Epoch 6/20\n",
      "6777/6777 [==============================] - 1s 219us/step - loss: 0.0765 - acc: 0.9779 - val_loss: 0.3309 - val_acc: 0.8899\n",
      "Epoch 7/20\n",
      "6777/6777 [==============================] - 1s 221us/step - loss: 0.0632 - acc: 0.9835 - val_loss: 0.3825 - val_acc: 0.8820\n",
      "Epoch 8/20\n",
      "6777/6777 [==============================] - 1s 221us/step - loss: 0.0631 - acc: 0.9827 - val_loss: 0.3578 - val_acc: 0.8833\n",
      "Epoch 9/20\n",
      "6777/6777 [==============================] - 2s 222us/step - loss: 0.0537 - acc: 0.9864 - val_loss: 0.3730 - val_acc: 0.8873\n",
      "Epoch 10/20\n",
      "6777/6777 [==============================] - 2s 223us/step - loss: 0.0400 - acc: 0.9882 - val_loss: 0.3926 - val_acc: 0.8859\n",
      "Epoch 11/20\n",
      "6777/6777 [==============================] - 2s 223us/step - loss: 0.0406 - acc: 0.9897 - val_loss: 0.4039 - val_acc: 0.8886\n",
      "Epoch 12/20\n",
      "6777/6777 [==============================] - 2s 224us/step - loss: 0.0412 - acc: 0.9889 - val_loss: 0.4165 - val_acc: 0.8806\n",
      "Epoch 13/20\n",
      "6777/6777 [==============================] - 2s 232us/step - loss: 0.0480 - acc: 0.9898 - val_loss: 0.4372 - val_acc: 0.8806\n",
      "Epoch 14/20\n",
      "6777/6777 [==============================] - 2s 236us/step - loss: 0.0367 - acc: 0.9901 - val_loss: 0.4414 - val_acc: 0.8833\n",
      "Epoch 15/20\n",
      "6777/6777 [==============================] - 2s 223us/step - loss: 0.0373 - acc: 0.9903 - val_loss: 0.4585 - val_acc: 0.8899\n",
      "Epoch 16/20\n",
      "6777/6777 [==============================] - 1s 221us/step - loss: 0.0347 - acc: 0.9900 - val_loss: 0.4456 - val_acc: 0.8859\n",
      "Epoch 17/20\n",
      "6777/6777 [==============================] - 1s 221us/step - loss: 0.0335 - acc: 0.9911 - val_loss: 0.4583 - val_acc: 0.8886\n",
      "Epoch 18/20\n",
      "6777/6777 [==============================] - 2s 223us/step - loss: 0.0387 - acc: 0.9894 - val_loss: 0.4417 - val_acc: 0.8873\n",
      "Epoch 19/20\n",
      "6777/6777 [==============================] - 2s 222us/step - loss: 0.0334 - acc: 0.9914 - val_loss: 0.4712 - val_acc: 0.8820\n",
      "Epoch 20/20\n",
      "6777/6777 [==============================] - 1s 220us/step - loss: 0.0333 - acc: 0.9895 - val_loss: 0.4598 - val_acc: 0.8873\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "train_size = int(len(df) * .7)\n",
    "train_posts = df['post'][:train_size]\n",
    "train_tags = df['tags'][:train_size]\n",
    "\n",
    "test_posts = df['post'][train_size:]\n",
    "test_tags = df['tags'][train_size:]\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3228/3228 [==============================] - 0s 28us/step\n",
      "Test accuracy: 0.8708178440877496\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
