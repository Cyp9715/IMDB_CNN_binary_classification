{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237 1237\n",
      "tt0804552\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1250, 32)          64000     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1250, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 625, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               5000250   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 5,067,605\n",
      "Trainable params: 5,067,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 11790 samples, validate on 56 samples\n",
      "Epoch 1/50\n",
      "11790/11790 [==============================] - 9s 729us/step - loss: 0.3596 - acc: 0.8250 - val_loss: 0.4825 - val_acc: 0.7321\n",
      "Epoch 2/50\n",
      "  256/11790 [..............................] - ETA: 7s - loss: 0.1719 - acc: 0.9336"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `accuracy` which is not available. Available metrics are: val_loss,val_acc,loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5888/11790 [=============>................] - ETA: 3s - loss: 0.1249 - acc: 0.9521"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f2bb8ddb5d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mtconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTCONST\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mgenres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGENRES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgarbage_remove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtconst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f2bb8ddb5d09>\u001b[0m in \u001b[0;36mAI\u001b[0;34m(preprocessing)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# EarlyStopping 을 설정하여 모델을 조기에 종료 시킬 수 있습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 조기종료 콜백함수 정의\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# 말씀드린 .h5파일입니다. 해당파일을 START_AND_END.py 에서 실행했을때 데이터를 바꾸어 보면 (tt000000 값) 학습할 결과와 동일하게 나오지 않습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Develop/anaconda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Develop/anaconda/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Develop/anaconda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import csv\n",
    "import numpy\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "\n",
    "\n",
    "# 1. 영화의 고유 번호와 장르를 읽어오는 단입니다, 호러, 로맨스, 액션 등등 원하는 장르의 영화만 골라올 수 있습니다.\n",
    "# 2. 중복되는 영화 장르가 있으므로 (Action, Comedy, Horror) 영화 장르간 우선권을 두어야 합니다. [genres[i].find(\"Romance\") == -1 and ...]\n",
    "# 3. 이 분류 방법은 한계점이 있습니다, \n",
    "#   같은 (Horror, Comedy) 장르의 영화가 있을 때, Horror 중심의 코미디를 가미한 영화가 있는 반면 \\\n",
    "#   Comedy 중심의 호러를 가미한 영화가 있는데, 이 딥러닝 모델은 해당 사실까지는 판별하지 못합니다.\n",
    "#   오로지 해당 영화가 얼마나 잔인할까, 아니면 공포스러울가를 맞추는 데에 촛점이 맞추어져 있습니다. (정확히는 대표작들과 비슷한가)\n",
    "def sequenceGenres():\n",
    "    f = open(r'./Basic_DATA/Movie_Genres.tsv', 'r', encoding='utf-8')\n",
    "    rdr = csv.reader(f, delimiter='\\t')\n",
    "\n",
    "    #영화, 출력 tconst, 출력 장르값\n",
    "    movie, output_tconst, output_genres = [] ,[], []\n",
    "    for i in rdr:\n",
    "        movie.append(i)\n",
    "\n",
    "    for i in range(movie.__len__()):\n",
    "    #   if movie[i][1].find(\"Horror\") == -1 and movie[i][1].find(\"Romance\")>=0:\n",
    "\n",
    "        if movie[i][1].find(\"Horror\") >= 0:\n",
    "            output_tconst.append(movie[i][0])\n",
    "            output_genres.append(movie[i][1])\n",
    "    \n",
    "    print(output_tconst.__len__(), output_genres.__len__())\n",
    "    \n",
    "    return output_tconst, output_genres\n",
    "\n",
    "# 트레인과 테스트 쓸모없는 값을 제거하는 단입니다, 현재 불용어를 제거하지 않은 상태입니다 (Movie, File)\n",
    "# 불용어를 제거하게 되면     t = Tokenizer(num_words=1000) 값을 더 줄이더라도 유의미한 값을 얻을 수 있을 것으로 예상됩니다.\n",
    "# t = Tokenizer(num_words=1000) 값을 줄이게 되면 GPU의 메모리 부하량이 줄어듭니다.\n",
    "#추 후 라이브러리를 통해 불용어를 제거할 예정입니다.\n",
    "def garbage_remove(tconst):\n",
    "    train = pd.read_csv(r'./Horror set/Horror.csv', skiprows=1, names=['Text', 'sentiment'])\n",
    "    test = pd.read_csv(r\"./Web_Crowling/\" + tconst + \".tsv\",sep='\\t', names=['Rank', 'ID', 'Title','Text','sentiment'])\n",
    "    print(tconst)\n",
    "    A, B = [], []\n",
    "\n",
    "    for i in train['Text']:\n",
    "        i = re.sub('[^a-zA-Z0-9 \\n]', '', str(i))\n",
    "        i = i.replace(\"br\",\"\").replace(\"  \",\" \").replace(\"   \",\" \")\n",
    "        A.append(i)\n",
    "\n",
    "    for i in test['Text']:\n",
    "        i = re.sub('[^a-zA-Z0-9 \\n]', '', str(i))\n",
    "        i = i.replace(\"br\",\"\").replace(\"  \",\" \").replace(\"   \",\" \")\n",
    "        B.append(i)\n",
    "\n",
    "    train['Text'] = A\n",
    "    test['Text'] = B\n",
    "    return train,test\n",
    "\n",
    "\n",
    "#쓰레기 값을 제거한 데이터들에 대해서, 백터화-웟 핫 인코딩을 한 뒤, 반환합니다.\n",
    "def preprocessing(garbage_remove):\n",
    "    t = Tokenizer(num_words=1250)\n",
    "    train,test = garbage_remove\n",
    "    test_label, train_label = [], []\n",
    "\n",
    "    t.fit_on_texts(train['Text'])\n",
    "    t.fit_on_texts(test['Text'])\n",
    "\n",
    "    #cruel = 1, astonish = 0 로 설정합니다.\n",
    "    for i in train['sentiment']:\n",
    "        if i == 'cruel':\n",
    "            train_label.append(1)\n",
    "        else:\n",
    "            train_label.append(0)\n",
    "    \n",
    "    #(이 부분 고려 필요) test_label 을 모두 cruel 로 설정, 정확도가 떨어질수록 astonish에 가깝다는 논지로, 상당히 정확합니다.\n",
    "    # 다만 .h5파일로 저장하여 불러들인뒤, 다른 데이터 셋(B)을 추가하여 확인하면\n",
    "    # 다른 데이터셋 (B)를 Test 데이터로 둔 것보다 훨신 떨어지는 결과를 얻습니다.\n",
    "    for i in test['sentiment']:\n",
    "            test_label.append(1)\n",
    "\n",
    "    train_result = t.texts_to_sequences(train['Text'])\n",
    "    test_result = t.texts_to_sequences(test['Text'])\n",
    "\n",
    "    # 원 핫 인코딩\n",
    "    train_result = t.sequences_to_matrix(train_result, mode='binary')\n",
    "    test_result = t.sequences_to_matrix(test_result, mode='binary')\n",
    "    return train_result, test_result, train_label, test_label\n",
    "\n",
    "#실질적인 모델 구성입니다. CNN 모델로 수성되어 있습니다\n",
    "def AI(preprocessing):\n",
    "    X_train,X_test,y_train,y_test = preprocessing\n",
    "\n",
    "    top_words = 2000\n",
    "    max_words = 1250\n",
    "    # amx_words 를 1000 이상으로 설정해 주어야 비교적 정확한 값을 얻을 수 있습니다. 임의적으로 줄이면 그래픽카드 메모리가 적어도 상관은 없지만.\n",
    "    # 30~70 사이의 값은 잔차가 굉장히 많이 튀게 됩니다.\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #model = multi_gpu_model(model, gpus=2) # 멀티 GPU를 사용하는 단입니다.\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    # EarlyStopping 을 설정하여 모델을 조기에 종료 시킬 수 있습니다.\n",
    "    early_stopping = EarlyStopping(monitor='accuracy', min_delta=0, patience=3, verbose=0, mode='auto') # 조기종료 콜백함수 정의\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128, verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # 말씀드린 .h5파일입니다. 해당파일을 START_AND_END.py 에서 실행했을때 데이터를 바꾸어 보면 (tt000000 값) 학습할 결과와 동일하게 나오지 않습니다.\n",
    "    # 추측되는 바로는 keras의 Tokenizer 문제일 가능성이 있습니다.\n",
    "    # 원래 이게 정상이고 제가 잘못 이해하고 있을지도 모릅니다.\n",
    "    # model.save('Horror 훈련셋\\\\horror_model.h5')\n",
    "    \n",
    "    # Final evaluation of the model\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def write(tconst, genres, scores):\n",
    "    f = open(r\"./Basic_DATA/Horror.tsv\", 'a', encoding='UTF8')\n",
    "    data = str(tconst) + \"\\t\" + str(genres) + \"\\t\" + \"Horror\" + \"\\t\" + str(numpy.round(scores[1]*100,2)) + \"\\n\"\n",
    "    print(\"data = \",data)\n",
    "    f.write(data)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "\n",
    "#score = AI(preprocessing(garbage_remove()))\n",
    "TCONST,GENRES = sequenceGenres()\n",
    "\n",
    "for i in range(607,len(TCONST)):\n",
    "    tconst = TCONST[i]\n",
    "    genres = GENRES[i]\n",
    "    scores = AI(preprocessing(garbage_remove(tconst)))\n",
    "    keras.backend.clear_session()\n",
    "    write(tconst, genres, scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1778 1778\n",
      "tt1454505\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1250, 32)          64000     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1250, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 625, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               5000250   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 5,067,605\n",
      "Trainable params: 5,067,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 9584 samples, validate on 11 samples\n",
      "Epoch 1/50\n",
      "1664/9584 [====>.........................] - ETA: 11s - loss: 0.6718 - acc: 0.6160"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-86ce3caa7dad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mtconst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTCONST\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[0mgenres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGENRES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgarbage_remove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtconst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtconst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-86ce3caa7dad>\u001b[0m in \u001b[0;36mAI\u001b[1;34m(preprocessing)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;31m# EarlyStopping 을 설정하여 모델을 조기에 종료 시킬 수 있습니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 조기종료 콜백함수 정의\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;31m# 말씀드린 .h5파일입니다. 해당파일을 START_AND_END.py 에서 실행했을때 데이터를 바꾸어 보면 (tt000000 값) 학습할 결과와 동일하게 나오지 않습니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import csv\n",
    "import numpy\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "\n",
    "\n",
    "# 1. 영화의 고유 번호와 장르를 읽어오는 단입니다, 호러, 로맨스, 액션 등등 원하는 장르의 영화만 골라올 수 있습니다.\n",
    "# 2. 중복되는 영화 장르가 있으므로 (Action, Comedy, Horror) 영화 장르간 우선권을 두어야 합니다. [genres[i].find(\"Romance\") == -1 and ...]\n",
    "# 3. 이 분류 방법은 한계점이 있습니다, \n",
    "#   같은 (Horror, Comedy) 장르의 영화가 있을 때, Horror 중심의 코미디를 가미한 영화가 있는 반면 \\\n",
    "#   Comedy 중심의 호러를 가미한 영화가 있는데, 이 딥러닝 모델은 해당 사실까지는 판별하지 못합니다.\n",
    "#   오로지 해당 영화가 얼마나 잔인할까, 아니면 공포스러울가를 맞추는 데에 촛점이 맞추어져 있습니다. (정확히는 대표작들과 비슷한가)\n",
    "def sequenceGenres():\n",
    "    f = open(r'./Basic_DATA/Movie_Genres.tsv', 'r', encoding='utf-8')\n",
    "    rdr = csv.reader(f, delimiter='\\t')\n",
    "\n",
    "    #영화, 출력 tconst, 출력 장르값\n",
    "    movie, output_tconst, output_genres = [] ,[], []\n",
    "    for i in rdr:\n",
    "        movie.append(i)\n",
    "\n",
    "    for i in range(movie.__len__()):\n",
    "        if movie[i][1].find(\"Horror\") == -1 and movie[i][1].find(\"Romance\")>=0:\n",
    "            output_tconst.append(movie[i][0])\n",
    "            output_genres.append(movie[i][1])\n",
    "    print(output_tconst.__len__(), output_genres.__len__())\n",
    "    \n",
    "    return output_tconst, output_genres\n",
    "\n",
    "# 트레인과 테스트 쓸모없는 값을 제거하는 단입니다, 현재 불용어를 제거하지 않은 상태입니다 (Movie, File)\n",
    "# 불용어를 제거하게 되면     t = Tokenizer(num_words=1000) 값을 더 줄이더라도 유의미한 값을 얻을 수 있을 것으로 예상됩니다.\n",
    "# t = Tokenizer(num_words=1000) 값을 줄이게 되면 GPU의 메모리 부하량이 줄어듭니다.\n",
    "#추 후 라이브러리를 통해 불용어를 제거할 예정입니다.\n",
    "def garbage_remove(tconst):\n",
    "    train = pd.read_csv(r'./Romance set/Romance.csv', skiprows=1, names=['Text', 'sentiment'])\n",
    "    test = pd.read_csv(r\"Web_Crowling/\" + tconst + \".tsv\",sep='\\t', names=['Rank', 'ID', 'Title','Text','sentiment'])\n",
    "    print(tconst)\n",
    "    A, B = [], []\n",
    "\n",
    "    for i in train['Text']:\n",
    "        i = re.sub('[^a-zA-Z0-9 \\n]', '', str(i))\n",
    "        i = i.replace(\"br\",\"\").replace(\"  \",\" \").replace(\"   \",\" \")\n",
    "        A.append(i)\n",
    "\n",
    "    for i in test['Text']:\n",
    "        i = re.sub('[^a-zA-Z0-9 \\n]', '', str(i))\n",
    "        i = i.replace(\"br\",\"\").replace(\"  \",\" \").replace(\"   \",\" \")\n",
    "        B.append(i)\n",
    "\n",
    "    train['Text'] = A\n",
    "    test['Text'] = B\n",
    "    return train,test\n",
    "\n",
    "\n",
    "#쓰레기 값을 제거한 데이터들에 대해서, 백터화-웟 핫 인코딩을 한 뒤, 반환합니다.\n",
    "def preprocessing(garbage_remove):\n",
    "    t = Tokenizer(num_words=1250)\n",
    "    train,test = garbage_remove\n",
    "    test_label, train_label = [], []\n",
    "\n",
    "    t.fit_on_texts(train['Text'])\n",
    "    t.fit_on_texts(test['Text'])\n",
    "\n",
    "    #cruel = 1, astonish = 0 로 설정합니다.\n",
    "    for i in train['sentiment']:\n",
    "        if i == 'happy':\n",
    "            train_label.append(1)\n",
    "        else:\n",
    "            train_label.append(0)\n",
    "    \n",
    "    #(이 부분 고려 필요) test_label 을 모두 cruel 로 설정, 정확도가 떨어질수록 astonish에 가깝다는 논지로, 상당히 정확합니다.\n",
    "    # 다만 .h5파일로 저장하여 불러들인뒤, 다른 데이터 셋(B)을 추가하여 확인하면\n",
    "    # 다른 데이터셋 (B)를 Test 데이터로 둔 것보다 훨신 떨어지는 결과를 얻습니다.\n",
    "    for i in test['sentiment']:\n",
    "            test_label.append(1)\n",
    "\n",
    "    train_result = t.texts_to_sequences(train['Text'])\n",
    "    test_result = t.texts_to_sequences(test['Text'])\n",
    "\n",
    "    # 원 핫 인코딩\n",
    "    train_result = t.sequences_to_matrix(train_result, mode='binary')\n",
    "    test_result = t.sequences_to_matrix(test_result, mode='binary')\n",
    "    return train_result, test_result, train_label, test_label\n",
    "\n",
    "#실질적인 모델 구성입니다. CNN 모델로 수성되어 있습니다\n",
    "def AI(preprocessing):\n",
    "    X_train,X_test,y_train,y_test = preprocessing\n",
    "\n",
    "    top_words = 2000\n",
    "    max_words = 1250\n",
    "    # amx_words 를 1000 이상으로 설정해 주어야 비교적 정확한 값을 얻을 수 있습니다. 임의적으로 줄이면 그래픽카드 메모리가 적어도 상관은 없지만.\n",
    "    # 30~70 사이의 값은 잔차가 굉장히 많이 튀게 됩니다.\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #model = multi_gpu_model(model, gpus=2) # 멀티 GPU를 사용하는 단입니다.\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    # EarlyStopping 을 설정하여 모델을 조기에 종료 시킬 수 있습니다.\n",
    "    early_stopping = EarlyStopping(monitor='accuracy', min_delta=0, patience=3, verbose=0, mode='auto') # 조기종료 콜백함수 정의\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128, verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # 말씀드린 .h5파일입니다. 해당파일을 START_AND_END.py 에서 실행했을때 데이터를 바꾸어 보면 (tt000000 값) 학습할 결과와 동일하게 나오지 않습니다.\n",
    "    # 추측되는 바로는 keras의 Tokenizer 문제일 가능성이 있습니다.\n",
    "    # 원래 이게 정상이고 제가 잘못 이해하고 있을지도 모릅니다.\n",
    "    # model.save('Horror 훈련셋\\\\horror_model.h5')\n",
    "    \n",
    "    # Final evaluation of the model\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def write(tconst, genres, scores):\n",
    "    f = open(r\"./Basic_DATA/Romance.tsv\", 'a', encoding='UTF8')\n",
    "    data = str(tconst) + \"\\t\" + str(genres) + \"\\t\" + \"Romance\" + \"\\t\" + str(numpy.round(scores[1]*100,2)) + \"\\n\"\n",
    "    print(\"data = \",data)\n",
    "    f.write(data)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "\n",
    "#score = AI(preprocessing(garbage_remove()))\n",
    "TCONST,GENRES = sequenceGenres()\n",
    "\n",
    "for i in range(1315,len(TCONST)):\n",
    "    tconst = TCONST[i]\n",
    "    genres = GENRES[i]\n",
    "    scores = AI(preprocessing(garbage_remove(tconst)))\n",
    "    keras.backend.clear_session()\n",
    "    write(tconst, genres, scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 44.,  56.],\n",
       "       [ 98., 128.]], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# 그래프를 생성합니다.\n",
    "c = []\n",
    "for d in ['/gpu:0', '/gpu:1']:\n",
    "    with tf.device(d):\n",
    "        a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n",
    "        b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n",
    "        c.append(tf.matmul(a, b))\n",
    "with tf.device('/cpu:0'):\n",
    "    sum = tf.add_n(c)\n",
    "# log_device_placement을 True로 설정하여 세션을 만듭니다.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# op를 실행합니다.\n",
    "sess.run(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
