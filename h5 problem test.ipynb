{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tt4361050\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import csv\n",
    "import numpy\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "\n",
    "\n",
    "def garbage_remove(tconst):\n",
    "    train = pd.read_csv(r'./Horror set/Horror.csv', skiprows=1, names=['Text', 'sentiment'])\n",
    "    test = pd.read_csv(r\"./Web_Crowling/\" + tconst + \".tsv\",sep='\\t', names=['Rank', 'ID', 'Title','Text','sentiment'])\n",
    "    print(tconst)\n",
    "    A, B = [], []\n",
    "\n",
    "    for i in train['Text']:\n",
    "        i = re.sub('[^a-zA-Z0-9 \\n]', '', str(i))\n",
    "        i = i.replace(\"br\",\"\").replace(\"  \",\" \").replace(\"   \",\" \")\n",
    "        A.append(i)\n",
    "\n",
    "    for i in test['Text']:\n",
    "        i = re.sub('[^a-zA-Z0-9 \\n]', '', str(i))\n",
    "        i = i.replace(\"br\",\"\").replace(\"  \",\" \").replace(\"   \",\" \")\n",
    "        B.append(i)\n",
    "\n",
    "    train['Text'] = A\n",
    "    test['Text'] = B\n",
    "    return train,test\n",
    "\n",
    "\n",
    "#쓰레기 값을 제거한 데이터들에 대해서, 백터화-웟 핫 인코딩을 한 뒤, 반환합니다.\n",
    "def preprocessing(garbage_remove):\n",
    "    t = Tokenizer(num_words=1250)\n",
    "    train,test = garbage_remove\n",
    "    test_label, train_label = [], []\n",
    "\n",
    "    t.fit_on_texts(train['Text'])\n",
    "    t.fit_on_texts(test['Text'])\n",
    "\n",
    "    #cruel = 1, astonish = 0 로 설정합니다.\n",
    "    for i in train['sentiment']:\n",
    "        if i == 'cruel':\n",
    "            train_label.append(1)\n",
    "        else:\n",
    "            train_label.append(0)\n",
    "    \n",
    "    #(이 부분 고려 필요) test_label 을 모두 cruel 로 설정, 정확도가 떨어질수록 astonish에 가깝다는 논지로, 상당히 정확합니다.\n",
    "    # 다만 .h5파일로 저장하여 불러들인뒤, 다른 데이터 셋(B)을 추가하여 확인하면\n",
    "    # 다른 데이터셋 (B)를 Test 데이터로 둔 것보다 훨신 떨어지는 결과를 얻습니다.\n",
    "    for i in test['sentiment']:\n",
    "            test_label.append(1)\n",
    "\n",
    "    train_result = t.texts_to_sequences(train['Text'])\n",
    "    test_result = t.texts_to_sequences(test['Text'])\n",
    "\n",
    "    # 원 핫 인코딩\n",
    "    train_result = t.sequences_to_matrix(train_result, mode='binary')\n",
    "    test_result = t.sequences_to_matrix(test_result, mode='binary')\n",
    "    return train_result, test_result, train_label, test_label\n",
    "\n",
    "\n",
    "tconst = \"tt4361050\"\n",
    "\n",
    "X_train,X_test,y_train,y_test = preprocessing(garbage_remove(tconst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원 핫 인코딩 처리된 X_test배열을 한번 확인 해 봅니다. (큰 의미는 없지만 추후 교차 검증을 거칠때 필요하기도 합니다..)\n",
    "\n",
    "X_test[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cyp/Develop/anaconda/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1250, 32)          64000     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1250, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 625, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               5000250   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 5,067,605\n",
      "Trainable params: 5,067,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[0.02763171]\n"
     ]
    }
   ],
   "source": [
    "#위에서 저장된 모델을 통해 X_test 값을 다시 넣어 보아, 정상적으로 작동하는지 확인합니다.\n",
    "model = load_model('Ouija.h5')\n",
    "model.summary()\n",
    "\n",
    "a = model.predict(X_test)\n",
    "k = 0\n",
    "for i in a:\n",
    "    k += i\n",
    "\n",
    "print(k/len(a))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
